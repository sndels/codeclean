Project diary for codeclean

Basic functionality
--------------------------------------------------------------------------------
I'll start by implementing a bare-bones version of comment removal from a single
input file to refine it before diving into child processes. This will be done as
a single function taking a filename as its argument and the implementation is
likely split to reasonable chunks after a working version is formed. The basic
architecture will be minimal at this point as I've yet to familiarize myself with
the boilerplate needed for the "additional" features of the task.

The cleaning function developed into reading a fixed length of characters and
keeping a simple state to handle longer lines and comment blocks. At this point,
the program takes a single file as a command line and removes comments as well
as empty lines (first character '\n') from it. Comments spanning over read buf-
fers and starting or ending at the border of buffers are also handled as needed.
A log file is created for operations of interest (beginnings, ends of comments,
detection of empty lines).

After implementing the buffered read with fgets I reconsidered the requirements
and decided to move straight to a memory mapped implementation for both the input
and output. This resulted in drastically simplified logic as reading was done
character by character with no more worries of overhead in reading that way. The
only real error case I'm currently aware of is string handling as comment tags
inside strings are treated as comments. This seems to be in line with the assign-
ment spec, which I'll have to double check.

Before moving onto processes, I split the code in more manageable pieces moved
logging to stdout and stderr. The reasoning behind the latter was that both
streams can be redirected to open files per process and not having to juggle a
log file in subroutines cleaned up the code quite nicely.

Child processes
--------------------------------------------------------------------------------
I continued to basic process handling. My aim is to have the main execution spawn
child processes to handle each file and possibly collect the logs in the main
process. I started trying out fork() by simply spawning processes in a loop and
printing the connected variables before exit().

Extending the code to having a process for each file was pretty straight forward
even with the redirection of stdout and stderr to a log file. I had to remove
buffering from stdout as redirecting it to a file without that caused the print
order to go out of whack when using both stdout and stderr. There seems to be
something wrong in my implementation as the program hangs or exits before subp-
rocesses if I print to stdout from multiple subprocesses after restoring stdout
and stderr to terminal. Possibly due to dup2 to and from the same file descriptor
for both streams? - Actually just a 'if (pid < 0)' for child branch instead of
if '(pid == 0)' as it shoud be. Derp.

Spawning a unique process for each file is probably not the smartest choice so I
adopted a structure where given arguments are divided into even blocks for the
child processes and the parent handles any trailing arguments. The amount of pro-
cesses generated is capped by either the amount of even argument blocks or a cons-
tant maximum child count, whichever is smaller. I also defined a single file as
a special case that only uses the parent process to simplify the block math.

Intermediate cleanup
-------------------------------------------------------------------------------
I asked about some vague parts of the spec and went to implementing the removal
of lines containing only whitespace. I simply used a flag and the position of the
first character in the current line to print e.g. indentation when actual code is
encountered. I also noticed that printing of the final character in a file was
broken when the second to last line ended in a comment. The obvious fix was to
check if the last two characters ended a comment block and print the last char-
acter if not.

So far testing has been through running cleanup on the source files of the project
which also captured the error in not handling the last character correctly. I've
adopted a habit of marking preprocessor endifs with the corresponding if clause
so both cleaned headers ended incorrectly. At this point, the cleanup seems to be
funcitonal as the cleaned sources compile and resulting executable works as
intended. I tested the forking of processes and handling of multiple input files
simply by comparing the input to relevant variables printed from each process.

Signal handling
-------------------------------------------------------------------------------
I pondered on the best place to implement handling SIGINT and figured it makes
most sense in the cleaning function as the main cleaning loop is inside it. I took
the example code provided in the lecture material and removed SIGIO from it as
that wasn't necessary for my purposes. The possibility of interrupting cleanup in
any state also helped me figure out a previously mishandled edgecase of the source
ending in a line comment. I settled on defining the interrupt variable as 0 or 2
so I could detect both the reason (file ended or SIGINT) for cleanup and possible
error encountered just by the summed return value of the function (0 = file ended,
1 = file ended + error, 2 = SIGINT, 3 = SIGINT + error). Thus, if the function
returned in response to SIGINT, the main loop also terminates.

Better stream handling
-------------------------------------------------------------------------------
I noticed that my current error handling in redirecting stdout and stderr is lack-
ing so I started to look into more robust checks. My current way of redirecting
both streams to the log file does enable printing detailed error descriptions using
perror but results in the inability to print errors to terminal while the streams
are redirected. Thus, I now redirect only stdout and do all logging to with printf
by using strerror to get error details. This keeps stderr intact so I can use that
for errors that should be seen in terminal. Failure to restore stdout now results
in the process in question to exit after printing the files left unprocessed.
Errors in creating or redirecting to the log_file just result in skipping the
current file after the error is printed to stderr. I also made logging and error
messages a bit more descriptive than they previously were.

File locking
-------------------------------------------------------------------------------
File locking is an obvious addition to the project so I again took the example
code and cut it down to a simple wrapper for fcntl. Then I implemented getting
read lock for input and write locks for output files with failure resulting in
relevant error messages and skipping the file.

Final testing and cleanup
-------------------------------------------------------------------------------
At this point, I have the required parts implemented so I move on to final cleanup
and testing of the code. I'll first do funcitonal testing by tailoring a couple
of test files for covering all the possible failure conditions I can think of +
hand cleaned references to compare against in software. I'll then do a few passes just
reading the code and commenting things while cleaning probable suboptimal lines
and running against the tests I made.
